// endpoint_selection.go - ç«¯ç‚¹é€‰æ‹©/è·¯ç”±åŠŸèƒ½
// åŒ…å«å¥åº·ç«¯ç‚¹è·å–ã€æ•…éšœè½¬ç§»ç«¯ç‚¹é€‰æ‹©ã€æ’åºç­–ç•¥ç­‰

package endpoint

import (
	"context"
	"fmt"
	"log/slog"
	"sort"
	"time"
)

// GetHealthyEndpoints returns a list of healthy endpoints from active groups based on strategy
// v5.0 Desktop: æ”¯æŒæ•…éšœè½¬ç§» - æ´»è·ƒç«¯ç‚¹ä¸å¥åº·æ—¶ï¼Œè¿”å›å…¶ä»– failover_enabled=true çš„å¥åº·ç«¯ç‚¹
func (m *Manager) GetHealthyEndpoints() []*Endpoint {
	// v5.0+: ä½¿ç”¨å¿«ç…§æœºåˆ¶
	m.endpointsMu.RLock()
	snapshot := make([]*Endpoint, len(m.endpoints))
	copy(snapshot, m.endpoints)
	m.endpointsMu.RUnlock()

	// 1. é¦–å…ˆå°è¯•è·å–æ´»è·ƒç»„ï¼ˆç”¨æˆ·æ¿€æ´»çš„ç«¯ç‚¹ï¼‰çš„å¥åº·ç«¯ç‚¹
	activeEndpoints := m.groupManager.FilterEndpointsByActiveGroups(snapshot)

	now := time.Now()
	var healthy []*Endpoint
	for _, endpoint := range activeEndpoints {
		endpoint.mutex.RLock()
		isHealthy := endpoint.Status.Healthy
		// æ£€æŸ¥æ˜¯å¦åœ¨è¯·æ±‚å†·å´ä¸­
		inCooldown := !endpoint.Status.CooldownUntil.IsZero() && now.Before(endpoint.Status.CooldownUntil)
		endpoint.mutex.RUnlock()

		if isHealthy && !inCooldown {
			healthy = append(healthy, endpoint)
		} else if inCooldown {
			slog.Debug(fmt.Sprintf("â­ï¸ [ç«¯ç‚¹é€‰æ‹©] è·³è¿‡å†·å´ä¸­çš„ç«¯ç‚¹: %s", endpoint.Config.Name))
		}
	}

	// 2. å¦‚æœæ´»è·ƒç«¯ç‚¹å¥åº·ä¸”ä¸åœ¨å†·å´ä¸­ï¼Œç›´æ¥è¿”å›
	if len(healthy) > 0 {
		return m.sortHealthyEndpoints(healthy, true)
	}

	// 3. æ´»è·ƒç«¯ç‚¹ä¸å¥åº·æˆ–åœ¨å†·å´ä¸­ï¼Œå°è¯•æ•…éšœè½¬ç§»
	if !m.config.Failover.Enabled {
		return healthy // æ•…éšœè½¬ç§»æœªå¯ç”¨ï¼Œè¿”å›ç©ºåˆ—è¡¨
	}

	slog.Info("ğŸ”„ [æ•…éšœè½¬ç§»] æ´»è·ƒç«¯ç‚¹ä¸å¯ç”¨ï¼ˆä¸å¥åº·æˆ–åœ¨å†·å´ä¸­ï¼‰ï¼Œå°è¯•æ•…éšœè½¬ç§»åˆ°å…¶ä»–ç«¯ç‚¹")
	healthy = m.getFailoverEndpoints(activeEndpoints, snapshot)

	if len(healthy) > 0 {
		slog.Info(fmt.Sprintf("âœ… [æ•…éšœè½¬ç§»] æ‰¾åˆ° %d ä¸ªå¯ç”¨çš„æ•…éšœè½¬ç§»ç«¯ç‚¹", len(healthy)))
	}

	return m.sortHealthyEndpoints(healthy, true) // æŒ‰ç­–ç•¥æ’åº
}

// getFailoverEndpoints è·å–æ•…éšœè½¬ç§»ç«¯ç‚¹ï¼ˆæ’é™¤æ´»è·ƒç«¯ç‚¹ï¼‰
// è¿”å›æ‰€æœ‰ failover_enabled=true ä¸”å¥åº·ä¸”ä¸åœ¨å†·å´ä¸­çš„éæ´»è·ƒç«¯ç‚¹
func (m *Manager) getFailoverEndpoints(activeEndpoints, snapshot []*Endpoint) []*Endpoint {
	// æ„å»ºæ´»è·ƒç«¯ç‚¹åç§°é›†åˆ
	activeNames := make(map[string]bool, len(activeEndpoints))
	for _, ep := range activeEndpoints {
		activeNames[ep.Config.Name] = true
	}

	now := time.Now()
	var failoverEndpoints []*Endpoint
	for _, endpoint := range snapshot {
		// è·³è¿‡æ´»è·ƒç«¯ç‚¹ï¼ˆå·²ç»æ£€æŸ¥è¿‡äº†ï¼‰
		if activeNames[endpoint.Config.Name] {
			continue
		}

		// æ£€æŸ¥æ˜¯å¦å‚ä¸æ•…éšœè½¬ç§»ï¼ˆé»˜è®¤ä¸º trueï¼‰
		failoverEnabled := true
		if endpoint.Config.FailoverEnabled != nil {
			failoverEnabled = *endpoint.Config.FailoverEnabled
		}
		if !failoverEnabled {
			continue
		}

		// æ£€æŸ¥å¥åº·çŠ¶æ€å’Œå†·å´çŠ¶æ€
		endpoint.mutex.RLock()
		isHealthy := endpoint.Status.Healthy
		inCooldown := !endpoint.Status.CooldownUntil.IsZero() && now.Before(endpoint.Status.CooldownUntil)
		endpoint.mutex.RUnlock()

		if inCooldown {
			slog.Debug(fmt.Sprintf("â­ï¸ [æ•…éšœè½¬ç§»] è·³è¿‡å†·å´ä¸­çš„ç«¯ç‚¹: %s", endpoint.Config.Name))
			continue
		}

		if isHealthy {
			failoverEndpoints = append(failoverEndpoints, endpoint)
		}
	}

	return failoverEndpoints
}

// sortHealthyEndpoints sorts healthy endpoints based on strategy with optional logging
func (m *Manager) sortHealthyEndpoints(healthy []*Endpoint, showLogs bool) []*Endpoint {
	// Sort based on strategy
	switch m.config.Strategy.Type {
	case "priority":
		sort.Slice(healthy, func(i, j int) bool {
			return healthy[i].Config.Priority < healthy[j].Config.Priority
		})
	case "fastest":
		// Log endpoint latencies for fastest strategy (only if showLogs is true)
		if len(healthy) > 1 && showLogs {
			slog.Info("ğŸ“Š [Fastest Strategy] åŸºäºå¥åº·æ£€æŸ¥çš„ç«¯ç‚¹å»¶è¿Ÿæ’åº:")
			for _, ep := range healthy {
				ep.mutex.RLock()
				responseTime := ep.Status.ResponseTime
				ep.mutex.RUnlock()
				slog.Info(fmt.Sprintf("  â±ï¸ %s - å»¶è¿Ÿ: %dms (æ¥æº: å®šæœŸå¥åº·æ£€æŸ¥)",
					ep.Config.Name, responseTime.Milliseconds()))
			}
		}

		sort.Slice(healthy, func(i, j int) bool {
			healthy[i].mutex.RLock()
			healthy[j].mutex.RLock()
			defer healthy[i].mutex.RUnlock()
			defer healthy[j].mutex.RUnlock()
			return healthy[i].Status.ResponseTime < healthy[j].Status.ResponseTime
		})
	}

	return healthy
}

// GetFastestEndpointsWithRealTimeTest returns endpoints from active groups sorted by real-time testing
// v5.0 Desktop: æ”¯æŒæ•…éšœè½¬ç§» - æ´»è·ƒç«¯ç‚¹ä¸å¥åº·æ—¶ï¼Œè¿”å›å…¶ä»– failover_enabled=true çš„å¥åº·ç«¯ç‚¹
func (m *Manager) GetFastestEndpointsWithRealTimeTest(ctx context.Context) []*Endpoint {
	// v5.0+: ä½¿ç”¨å¿«ç…§æœºåˆ¶
	m.endpointsMu.RLock()
	snapshot := make([]*Endpoint, len(m.endpoints))
	copy(snapshot, m.endpoints)
	m.endpointsMu.RUnlock()

	// 1. é¦–å…ˆå°è¯•è·å–æ´»è·ƒç»„ï¼ˆç”¨æˆ·æ¿€æ´»çš„ç«¯ç‚¹ï¼‰çš„å¥åº·ç«¯ç‚¹
	activeEndpoints := m.groupManager.FilterEndpointsByActiveGroups(snapshot)

	var healthy []*Endpoint
	for _, endpoint := range activeEndpoints {
		endpoint.mutex.RLock()
		if endpoint.Status.Healthy {
			healthy = append(healthy, endpoint)
		}
		endpoint.mutex.RUnlock()
	}

	// 2. å¦‚æœæ´»è·ƒç«¯ç‚¹ä¸å¥åº·ï¼Œå°è¯•æ•…éšœè½¬ç§»
	if len(healthy) == 0 && m.config.Failover.Enabled {
		slog.InfoContext(ctx, "ğŸ”„ [æ•…éšœè½¬ç§»] æ´»è·ƒç«¯ç‚¹ä¸å¥åº·ï¼Œå°è¯•æ•…éšœè½¬ç§»åˆ°å…¶ä»–ç«¯ç‚¹")
		healthy = m.getFailoverEndpoints(activeEndpoints, snapshot)

		if len(healthy) > 0 {
			slog.InfoContext(ctx, fmt.Sprintf("âœ… [æ•…éšœè½¬ç§»] æ‰¾åˆ° %d ä¸ªå¯ç”¨çš„æ•…éšœè½¬ç§»ç«¯ç‚¹", len(healthy)))
		}
	}

	if len(healthy) == 0 {
		return healthy
	}

	// If not using fastest strategy or fast test disabled, apply sorting with logging
	if m.config.Strategy.Type != "fastest" || !m.config.Strategy.FastTestEnabled {
		return m.sortHealthyEndpoints(healthy, true) // Show logs
	}

	// Check if we have cached fast test results first
	testResults, usedCache := m.fastTester.TestEndpointsParallel(ctx, healthy)

	// Only show health check sorting if we're NOT using cache
	if !usedCache && m.config.Strategy.Type == "fastest" && len(healthy) > 1 {
		slog.InfoContext(ctx, "ğŸ“Š [Fastest Strategy] åŸºäºå¥åº·æ£€æŸ¥çš„æ´»è·ƒç»„ç«¯ç‚¹å»¶è¿Ÿæ’åº:")
		for _, ep := range healthy {
			ep.mutex.RLock()
			responseTime := ep.Status.ResponseTime
			group := ep.Config.Group
			ep.mutex.RUnlock()
			slog.InfoContext(ctx, fmt.Sprintf("  â±ï¸ %s (ç»„: %s) - å»¶è¿Ÿ: %dms (æ¥æº: å®šæœŸå¥åº·æ£€æŸ¥)",
				ep.Config.Name, group, responseTime.Milliseconds()))
		}
	}

	// Log ALL test results first (including failures) - but only if cache wasn't used
	if len(testResults) > 0 && !usedCache {
		slog.InfoContext(ctx, "ğŸ” [Fastest Response Mode] æ´»è·ƒç»„ç«¯ç‚¹æ€§èƒ½æµ‹è¯•ç»“æœ:")
		successCount := 0
		for _, result := range testResults {
			group := result.Endpoint.Config.Group
			if result.Success {
				successCount++
				slog.InfoContext(ctx, fmt.Sprintf("  âœ… å¥åº· %s (ç»„: %s) - å“åº”æ—¶é—´: %dms",
					result.Endpoint.Config.Name, group,
					result.ResponseTime.Milliseconds()))
			} else {
				errorMsg := ""
				if result.Error != nil {
					errorMsg = fmt.Sprintf(" - é”™è¯¯: %s", result.Error.Error())
				}
				slog.InfoContext(ctx, fmt.Sprintf("  âŒ å¼‚å¸¸ %s (ç»„: %s) - å“åº”æ—¶é—´: %dms%s",
					result.Endpoint.Config.Name, group,
					result.ResponseTime.Milliseconds(),
					errorMsg))
			}
		}

		slog.InfoContext(ctx, fmt.Sprintf("ğŸ“Š [æµ‹è¯•æ‘˜è¦] æ´»è·ƒç»„æµ‹è¯•: %dä¸ªç«¯ç‚¹, å¥åº·: %dä¸ª, å¼‚å¸¸: %dä¸ª",
			len(testResults), successCount, len(testResults)-successCount))
	}

	// Sort by response time (only successful results)
	sortedResults := SortByResponseTime(testResults)

	if len(sortedResults) == 0 {
		slog.WarnContext(ctx, "âš ï¸ [Fastest Response Mode] æ´»è·ƒç»„æ‰€æœ‰ç«¯ç‚¹æµ‹è¯•å¤±è´¥ï¼Œå›é€€åˆ°å¥åº·æ£€æŸ¥æ¨¡å¼")
		return healthy // Fall back to health check results if no fast tests succeeded
	}

	// Convert back to endpoint slice
	endpoints := make([]*Endpoint, 0, len(sortedResults))
	for _, result := range sortedResults {
		endpoints = append(endpoints, result.Endpoint)
	}

	// Log the successful endpoint ranking
	if len(endpoints) > 0 {
		// Show the fastest endpoint selection
		fastestEndpoint := endpoints[0]
		var fastestTime int64
		for _, result := range sortedResults {
			if result.Endpoint == fastestEndpoint {
				fastestTime = result.ResponseTime.Milliseconds()
				break
			}
		}

		cacheIndicator := ""
		if usedCache {
			cacheIndicator = " (ç¼“å­˜)"
		}

		slog.InfoContext(ctx, fmt.Sprintf("ğŸš€ [Fastest Response Mode] é€‰æ‹©æœ€å¿«ç«¯ç‚¹: %s - %dms%s",
			fastestEndpoint.Config.Name, fastestTime, cacheIndicator))

		// Show other available endpoints if there are more than one
		if len(endpoints) > 1 && !usedCache {
			slog.InfoContext(ctx, "ğŸ“‹ [å¤‡ç”¨ç«¯ç‚¹] å…¶ä»–å¯ç”¨ç«¯ç‚¹:")
			for i := 1; i < len(endpoints); i++ {
				ep := endpoints[i]
				var responseTime int64
				var epGroup string
				for _, result := range sortedResults {
					if result.Endpoint == ep {
						responseTime = result.ResponseTime.Milliseconds()
						epGroup = result.Endpoint.Config.Group
						break
					}
				}
				slog.InfoContext(ctx, fmt.Sprintf("  ğŸ”„ å¤‡ç”¨ %s (ç»„: %s) - å“åº”æ—¶é—´: %dms",
					ep.Config.Name, epGroup, responseTime))
			}
		}
	}

	return endpoints
}

// GetEndpointByName returns an endpoint by name, only from active groups
func (m *Manager) GetEndpointByName(name string) *Endpoint {
	// v5.0+: ä½¿ç”¨å¿«ç…§æœºåˆ¶
	m.endpointsMu.RLock()
	snapshot := make([]*Endpoint, len(m.endpoints))
	copy(snapshot, m.endpoints)
	m.endpointsMu.RUnlock()

	// First filter by active groups
	activeEndpoints := m.groupManager.FilterEndpointsByActiveGroups(snapshot)

	// Then find by name
	for _, endpoint := range activeEndpoints {
		if endpoint.Config.Name == name {
			return endpoint
		}
	}
	return nil
}

// GetEndpointByNameAny returns an endpoint by name from all endpoints (ignoring group status)
func (m *Manager) GetEndpointByNameAny(name string) *Endpoint {
	m.endpointsMu.RLock()
	defer m.endpointsMu.RUnlock()

	for _, endpoint := range m.endpoints {
		if endpoint.Config.Name == name {
			return endpoint
		}
	}
	return nil
}

// GetAllEndpoints returns all endpoints (deprecated: use GetEndpoints instead)
func (m *Manager) GetAllEndpoints() []*Endpoint {
	m.endpointsMu.RLock()
	defer m.endpointsMu.RUnlock()

	result := make([]*Endpoint, len(m.endpoints))
	copy(result, m.endpoints)
	return result
}

// GetEndpoints returns all endpoints for Web interface
func (m *Manager) GetEndpoints() []*Endpoint {
	m.endpointsMu.RLock()
	defer m.endpointsMu.RUnlock()

	result := make([]*Endpoint, len(m.endpoints))
	copy(result, m.endpoints)
	return result
}

// GetEndpointStatus returns the status of an endpoint by name
func (m *Manager) GetEndpointStatus(name string) EndpointStatus {
	m.endpointsMu.RLock()
	defer m.endpointsMu.RUnlock()

	for _, ep := range m.endpoints {
		if ep.Config.Name == name {
			ep.mutex.RLock()
			status := ep.Status
			ep.mutex.RUnlock()
			return status
		}
	}
	return EndpointStatus{}
}

// GetEndpointCount è¿”å›å½“å‰ç«¯ç‚¹æ•°é‡ï¼ˆv5.0+ æ–°å¢ï¼‰
func (m *Manager) GetEndpointCount() int {
	m.endpointsMu.RLock()
	defer m.endpointsMu.RUnlock()
	return len(m.endpoints)
}
